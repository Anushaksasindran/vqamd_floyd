{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you haven't, please execute the following cell **once per Workspace Session** to install all the necessary requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash run_me_first_on_floyd.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Question Answering: Part II\n",
    "\n",
    "### LSTM Based Recurrent Model\n",
    "\n",
    "This notebook is simply an execution of the code to build VQA model based on a `CNN + LSTM` based approach, I would highly encourage you to read the [full post here](https://sominwadhwa.github.io/blog/2018/01/01/de/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://cdn-images-1.medium.com/max/1600/1*YkxweTzgt4axMnaTH0nHHg.gif\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's get all the necessary library imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from random import shuffle, sample\n",
    "import pickle as pk\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Reshape\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Merge\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from progressbar import Bar, ETA, Percentage, ProgressBar    \n",
    "from keras.models import model_from_json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import spacy\n",
    "#from spacy.en import English\n",
    "\n",
    "from src.utils import *\n",
    "from src.features import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessed Data\n",
    "\n",
    "The open-source VQA dataset contains multiple open-ended questions about various images. All my experiments were performed with v1 of the dataset (though I've processed v2 of the dataset as well), which contains:\n",
    "\n",
    "- 82,783 training images from COCO (common objects in context) dataset.\n",
    "- 215,407 question-answer pairs for training images.\n",
    "- 40,504 validation images to perform own testing.\n",
    "- 121,512 question-answer pairs for validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_questions     = open(\"preprocessed/v1/ques_train.txt\",\"rb\").read().decode('utf8').splitlines()\n",
    "training_questions_len = open(\"preprocessed/v1/ques_train_len.txt\",\"rb\").read().decode('utf8').splitlines()\n",
    "answers_train          = open(\"preprocessed/v1/answer_train.txt\",\"rb\").read().decode('utf8').splitlines()\n",
    "images_train           = open(\"preprocessed/v1/images_coco_id.txt\",\"rb\").read().decode('utf8').splitlines()\n",
    "img_ids                = open('preprocessed/v1/coco_vgg_IDMap.txt').read().splitlines()\n",
    "vgg_path               = \"/floyd/input/vqa_data/coco/vgg_feats.mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple of questions along with their answers. The first entry you see here is the **COCO Image ID** through with the image can be found at [http://cocodataset.org/#explore](http://cocodataset.org/#explore) by simply entering the image ID in the **search** column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('488792', 'Do you see any tall buildings?', 'yes'),\n",
       " ('336563', 'Is his left foot lifted?', 'yes'),\n",
       " ('508191',\n",
       "  \"What is the state on the motorcycle's license plate?\",\n",
       "  'california'),\n",
       " ('205826', 'Is there a lot of penguins in this picture?', 'yes'),\n",
       " ('61949', 'Are there any animals in the picture?', 'yes')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(list(zip(images_train, training_questions, answers_train)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded WordVec\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "print (\"Loaded WordVec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded VGG Weights\n"
     ]
    }
   ],
   "source": [
    "vgg_features = scipy.io.loadmat(vgg_path)\n",
    "img_features = vgg_features['feats']\n",
    "id_map = dict()\n",
    "print (\"Loaded VGG Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load image features - `4096` sized vectors extracted from the last layer of a VGG network trained on the COCO Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215407 215407 215407\n"
     ]
    }
   ],
   "source": [
    "upper_lim = 1000 #Number of most frequently occurring answers in COCOVQA (Covering >80% of the total data)\n",
    "training_questions, answers_train, images_train = freq_answers(training_questions, \n",
    "                                                               answers_train, images_train, upper_lim)\n",
    "training_questions_len, training_questions, answers_train, images_train = (list(t) for t in zip(*sorted(zip(training_questions_len, \n",
    "                                                                                                          training_questions, answers_train, \n",
    "                                                                                                          images_train))))\n",
    "print (len(training_questions), len(answers_train),len(images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = LabelEncoder()\n",
    "lbl.fit(answers_train)\n",
    "nb_classes = len(list(lbl.classes_))\n",
    "pk.dump(lbl, open('preprocessed/v1/label_encoder_lstm.sav','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size               =      256\n",
    "img_dim                  =     4096\n",
    "word2vec_dim             =      300\n",
    "#max_len                 =       30 # Required only when using Fixed-Length Padding\n",
    "\n",
    "num_hidden_nodes_mlp     =     1024\n",
    "num_hidden_nodes_lstm    =      512\n",
    "num_layers_mlp           =        3\n",
    "num_layers_lstm          =        3\n",
    "dropout                  =       0.5\n",
    "activation_mlp           =     'tanh'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_epochs`: Set to the number of epochs you'd wish to run the network for.\n",
    "\n",
    "`log_interval`: This parameter sets the epoch interval after which a copy of the model weights will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the following based on your usage, THESE WILL DIRECTLY AFFECT THE DURATION OF NETWORK TRAINING\n",
    "num_epochs               =         300 \n",
    "log_interval             =         15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in img_ids:\n",
    "    id_split = ids.split()\n",
    "    id_map[id_split[0]] = int(id_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 4096)              0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_model = Sequential()\n",
    "image_model.add(Reshape(input_shape = (img_dim,), target_shape=(img_dim,)))\n",
    "image_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 512)         1665024   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 512)         2099200   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 512)               2099200   \n",
      "=================================================================\n",
      "Total params: 5,863,424\n",
      "Trainable params: 5,863,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "language_model = Sequential()\n",
    "language_model.add(LSTM(output_dim=num_hidden_nodes_lstm, \n",
    "                        return_sequences=True, input_shape=(None, word2vec_dim)))\n",
    "\n",
    "for i in range(num_layers_lstm-2):\n",
    "    language_model.add(LSTM(output_dim=num_hidden_nodes_lstm, return_sequences=True))\n",
    "language_model.add(LSTM(output_dim=num_hidden_nodes_lstm, return_sequences=False))\n",
    "\n",
    "language_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Merge([language_model, image_model], mode='concat', concat_axis=1))\n",
    "for i in range(num_layers_mlp):\n",
    "    model.add(Dense(num_hidden_nodes_mlp, init='uniform'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "model.add(Dense(upper_lim))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5825"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dump = model.to_json()\n",
    "open('lstm_structure'  + '.json', 'w').write(model_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_1 (Merge)              (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              1025000   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 13,707,240\n",
      "Trainable params: 13,707,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And we're good to go!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215552/215407 [==============================] - 165s 767us/step - train loss: 4.2766\n",
      "215552/215407 [==============================] - 158s 732us/step - train loss: 3.7679\n",
      "215552/215407 [==============================] - 158s 734us/step - train loss: 3.6132\n",
      "215552/215407 [==============================] - 159s 738us/step - train loss: 3.4308\n",
      "215552/215407 [==============================] - 158s 732us/step - train loss: 3.3099\n",
      "215552/215407 [==============================] - 158s 732us/step - train loss: 3.2623\n",
      "215552/215407 [==============================] - 158s 732us/step - train loss: 3.2482\n",
      "215552/215407 [==============================] - 158s 731us/step - train loss: 3.2418\n",
      "215552/215407 [==============================] - 157s 731us/step - train loss: 3.1891\n",
      "215552/215407 [==============================] - 157s 730us/step - train loss: 3.1516\n",
      "215552/215407 [==============================] - 158s 733us/step - train loss: 3.1321\n",
      "215552/215407 [==============================] - 156s 723us/step - train loss: 3.0853\n",
      "215552/215407 [==============================] - 156s 723us/step - train loss: 2.9847\n",
      "215552/215407 [==============================] - 157s 730us/step - train loss: 2.9033\n",
      "215552/215407 [==============================] - 156s 725us/step - train loss: 2.8702\n",
      "215552/215407 [==============================] - 154s 714us/step - train loss: 2.8441\n",
      "215552/215407 [==============================] - 154s 712us/step - train loss: 2.8234\n",
      "215552/215407 [==============================] - 153s 712us/step - train loss: 2.8037\n",
      "215552/215407 [==============================] - 153s 711us/step - train loss: 2.7898\n",
      "215552/215407 [==============================] - 153s 712us/step - train loss: 2.7772\n",
      "215552/215407 [==============================] - 153s 710us/step - train loss: 2.7730\n",
      "215552/215407 [==============================] - 154s 712us/step - train loss: 2.7589\n",
      "215552/215407 [==============================] - 154s 716us/step - train loss: 2.7480\n",
      "215552/215407 [==============================] - 154s 716us/step - train loss: 2.7422\n",
      "215552/215407 [==============================] - 154s 715us/step - train loss: 2.7287\n",
      "215552/215407 [==============================] - 156s 722us/step - train loss: 2.7274\n",
      "215552/215407 [==============================] - 155s 721us/step - train loss: 2.7241\n",
      "215552/215407 [==============================] - 155s 718us/step - train loss: 2.7090\n",
      "215552/215407 [==============================] - 154s 715us/step - train loss: 2.7039\n",
      "215552/215407 [==============================] - 154s 716us/step - train loss: 2.7013\n",
      "215552/215407 [==============================] - 154s 715us/step - train loss: 2.6968\n",
      "215552/215407 [==============================] - 154s 714us/step - train loss: 2.6927\n",
      "215552/215407 [==============================] - 154s 714us/step - train loss: 2.6822\n",
      "215552/215407 [==============================] - 155s 718us/step - train loss: 2.6790\n",
      "215552/215407 [==============================] - 155s 717us/step - train loss: 2.6732\n",
      "215552/215407 [==============================] - 155s 719us/step - train loss: 2.6620\n",
      "215552/215407 [==============================] - 155s 718us/step - train loss: 2.6652\n",
      "215552/215407 [==============================] - 155s 717us/step - train loss: 2.6567\n",
      "215552/215407 [==============================] - 154s 717us/step - train loss: 2.6532\n",
      "215552/215407 [==============================] - 155s 717us/step - train loss: 2.6432\n",
      "215552/215407 [==============================] - 155s 717us/step - train loss: 2.6378\n",
      "215552/215407 [==============================] - 155s 719us/step - train loss: 2.6313\n",
      "215552/215407 [==============================] - 155s 718us/step - train loss: 2.6271\n",
      "215552/215407 [==============================] - 154s 714us/step - train loss: 2.6191\n",
      "215552/215407 [==============================] - 154s 713us/step - train loss: 2.6103\n",
      "215552/215407 [==============================] - 154s 713us/step - train loss: 2.6070\n",
      "215552/215407 [==============================] - 154s 716us/step - train loss: 2.6032\n",
      "215552/215407 [==============================] - 154s 715us/step - train loss: 2.6011\n",
      "215552/215407 [==============================] - 155s 717us/step - train loss: 2.5911\n",
      " 72448/215407 [=========>....................] - ETA: 1:42 - train loss: 2.4110"
     ]
    }
   ],
   "source": [
    "for k in range(num_epochs):\n",
    "    progbar = generic_utils.Progbar(len(training_questions))\n",
    "    for ques_batch, ans_batch, im_batch in zip(grouped(training_questions, batch_size, \n",
    "                                                       fillvalue=training_questions[-1]), \n",
    "                                               grouped(answers_train, batch_size, \n",
    "                                                       fillvalue=answers_train[-1]), \n",
    "                                               grouped(images_train, batch_size, fillvalue=images_train[-1])):\n",
    "        timestep = len(nlp(ques_batch[-1]))\n",
    "        X_ques_batch = get_questions_tensor_timeseries(ques_batch, nlp, timestep)\n",
    "        #print (X_ques_batch.shape)\n",
    "        X_img_batch = get_images_matrix(im_batch, id_map, img_features)\n",
    "        Y_batch = get_answers_sum(ans_batch, lbl)\n",
    "        loss = model.train_on_batch([X_ques_batch, X_img_batch], Y_batch)\n",
    "        progbar.add(batch_size, values=[('train loss', loss)])\n",
    "    if k%log_interval == 0:\n",
    "        model.save_weights(\"weights/LSTM\" + \"_epoch_{:02d}.hdf5\".format(k))\n",
    "model.save_weights(\"weights/LSTM\" + \"_epoch_{:02d}.hdf5\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's evaluate our model!\n",
    "\n",
    "We're going to evalute our model on the validation set provided by the **VQA Dataset** which I've already preprocessed much like our training datasets. Refer to [VQA Evaluation](http://visualqa.org/evaluation.html).\n",
    "\n",
    "While I have evaluated my pre-trained models over here, you might like to change the paths in order to evaluate your own models. This can be easily done in the following way -\n",
    "\n",
    "1. Add `model_from_json(open('lstm_structure.json').read())` (instead of loading model structure from my dataset, use the one you just created.\n",
    "2. Modify -> `model.load_weights(\"weights/<weights_file>\")` (instead of loading weights from my pretrained models, use your own set stored under `weights` directory.\n",
    "\n",
    "By default, we're going to load the weights & the model created at the beginning of your training loop (for testing purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sw/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1192: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/sw/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Model Loaded with Weights\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_1 (Merge)              (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              1025000   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 13,707,240\n",
      "Trainable params: 13,707,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_from_json(open('lstm_structure.json').read()) #fully trained model & weights present at /floyd/input/vqa_data/weights/\n",
    "# In case you wish to evaluate the model you just trained, uncomment the following line of code & comment out the subsequent one -\n",
    "#model.load_weights('weights/LSTM_epoch_00.hdf5')\n",
    "model.load_weights('/floyd/input/vqa_data/weights/LSTM_epoch_45.hdf5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print (\"Model Loaded with Weights\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the validation preprocessed data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs = open('preprocessed/v1/val_images_coco_id.txt','rb').read().decode('utf-8').splitlines()\n",
    "val_ques = open('preprocessed/v1/ques_val.txt','rb').read().decode('utf-8').splitlines()\n",
    "val_ans  = open('preprocessed/v1/answer_val.txt','rb').read().decode('utf-8').splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace location of LabelEncoder to your own, otherwise this may affect accuracy.** To do so, simply change the `file_path` to `preprocessed/v1/label_encoder_<type>.sav`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = pk.load(open('preprocessed/v1/label_encoder_lstm.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "batch_size = 128 \n",
    "\n",
    "#print (\"Word2Vec Loaded!\")\n",
    "\n",
    "widgets = ['Evaluating ', Percentage(), ' ', Bar(marker='#',left='[',right=']'), ' ', ETA()]\n",
    "pbar = ProgressBar(widgets=widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating N/A% [#                                             ] Time:  0:11:40\n"
     ]
    }
   ],
   "source": [
    "for qu_batch,an_batch,im_batch in pbar(zip(grouped(val_ques, batch_size, \n",
    "                                                   fillvalue=val_ques[0]), \n",
    "                                           grouped(val_ans, batch_size, \n",
    "                                                   fillvalue=val_ans[0]), \n",
    "                                           grouped(val_imgs, batch_size, \n",
    "                                                   fillvalue=val_imgs[0]))):\n",
    "    timesteps = len(nlp(qu_batch[-1]))\n",
    "    X_ques_batch = get_questions_tensor_timeseries(qu_batch, nlp, timesteps)\n",
    "    X_i_batch = get_images_matrix(im_batch, id_map, img_features)\n",
    "    X_batch = [X_ques_batch, X_i_batch]\n",
    "    y_predict = model.predict_classes(X_batch, verbose=0)\n",
    "    y_pred.extend(label_encoder.inverse_transform(y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  54.79\n"
     ]
    }
   ],
   "source": [
    "correct_val = 0.0\n",
    "total = 0\n",
    "f1 = open('res.txt','w')\n",
    "\n",
    "for pred, truth, ques, img in zip(y_pred, val_ans, val_ques, val_imgs):\n",
    "    t_count = 0\n",
    "    for _truth in truth.split(';'):\n",
    "        if pred == truth:\n",
    "            t_count += 1 \n",
    "    if t_count >=1:\n",
    "        correct_val +=1\n",
    "    else:\n",
    "        correct_val += float(t_count)/3\n",
    "\n",
    "    total +=1\n",
    "\n",
    "    try:\n",
    "        f1.write(str(ques))\n",
    "        f1.write('\\n')\n",
    "        f1.write(str(img))\n",
    "        f1.write('\\n')\n",
    "        f1.write(str(pred))\n",
    "        f1.write('\\n')\n",
    "        f1.write(str(truth))\n",
    "        f1.write('\\n')\n",
    "        f1.write('\\n')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print (\"Accuracy: \", round((correct_val/total)*100,2))\n",
    "#f1.write('Final Accuracy is ' + str(round(correct_val/total),2))\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The accuracy achieved here is quite close to what the authors of [VQA Paper](https://arxiv.org/pdf/1505.00468.pdf) have reported in their LSTM based approach.**\n",
    "\n",
    "There you go, all set to participate in the next VQA Challenge!\n",
    "\n",
    "If you do, however, would like to try out these models on your own custom images do checkout **`src/test.py`** with an image and a characterstic question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
