{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Question Answering: Part I\n",
    "\n",
    "### Baseline Approach: A Bag of Words Model\n",
    "\n",
    "This notebook is simply an execution of the code to build VQA model using a basic `Neural Network (Multilayer Perceptron) + Bag of Words`, I would highly encourage you to read the [full post here](https://sominwadhwa.github.io/blog/2018/01/01/de/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/sominwadhwa/sominwadhwa.github.io/blob/master/assets/vqa/5.jpg?raw=true\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's get all the necessary library imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from random import shuffle, sample\n",
    "import pickle as pk\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from progressbar import Bar, ETA, Percentage, ProgressBar    \n",
    "from keras.models import model_from_json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import spacy\n",
    "#from spacy.en import English\n",
    "\n",
    "from src.utils import *\n",
    "from src.features import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessed Data\n",
    "\n",
    "The open-source VQA dataset contains multiple open-ended questions about various images. All my experiments were performed with v1 of the dataset (though I've processed v2 of the dataset as well), which contains:\n",
    "\n",
    "- 82,783 training images from COCO (common objects in context) dataset.\n",
    "- 215,407 question-answer pairs for training images.\n",
    "- 40,504 validation images to perform own testing.\n",
    "- 121,512 question-answer pairs for validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_questions = open(\"preprocessed/v1/ques_train.txt\",\"rb\").read().decode('utf8').splitlines()\n",
    "answers_train      = open(\"preprocessed/v1/answer_train.txt\",\"rb\").read().decode('utf8').splitlines()\n",
    "images_train       = open(\"preprocessed/v1/images_coco_id.txt\",\"rb\").read().decode('utf8').splitlines()\n",
    "img_ids            = open('preprocessed/v1/coco_vgg_IDMap.txt').read().splitlines()\n",
    "vgg_path           = \"/floyd/input/vqa_data/coco/vgg_feats.mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple of questions along with their answers. The first entry you see here is the **COCO Image ID** through with the image can be found at [http://cocodataset.org/#explore](http://cocodataset.org/#explore) by simply entering the image ID in the **search** column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('354220', 'What are the elephants doing?', 'walking'),\n",
       " ('306440', 'What city is this?', 'new york'),\n",
       " ('68576', 'What are they riding in?', 'airplane'),\n",
       " ('384023', 'Is the train going through a city?', 'no'),\n",
       " ('269273', 'What nationality is the person in the picture?', 'asian')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(list(zip(images_train, training_questions, answers_train)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.4 s, sys: 808 ms, total: 24.2 s\n",
      "Wall time: 24.3 s\n",
      "Loaded WordVec\n"
     ]
    }
   ],
   "source": [
    "%time nlp = spacy.load(\"en_core_web_md\")\n",
    "print (\"Loaded WordVec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load image features - `4096` sized vectors extracted from the last layer of a VGG network trained on the COCO Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.53 s, sys: 2.57 s, total: 12.1 s\n",
      "Wall time: 12.1 s\n",
      "Loaded VGG Weights\n"
     ]
    }
   ],
   "source": [
    "%time vgg_features = scipy.io.loadmat(vgg_path)\n",
    "img_features = vgg_features['feats']\n",
    "id_map = dict()\n",
    "print (\"Loaded VGG Weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215407 215407 215407\n"
     ]
    }
   ],
   "source": [
    "upper_lim = 1000 #Number of most frequently occurring answers in COCOVQA (Coverting >85% of the total data)\n",
    "training_questions, answers_train, images_train = freq_answers(training_questions, answers_train, images_train, upper_lim)\n",
    "print (len(training_questions), len(answers_train),len(images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = LabelEncoder()\n",
    "lbl.fit(answers_train)\n",
    "nb_classes = len(list(lbl.classes_))\n",
    "pk.dump(lbl, open('preprocessed/v1/label_encoder_mlp.sav','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_units  = 1024\n",
    "num_hidden_layers = 3\n",
    "batch_size        = 128\n",
    "dropout           = 0.5\n",
    "activation        = 'tanh'\n",
    "img_dim           = 4096\n",
    "word2vec_dim      = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_epochs`: Set to the number of epochs you'd wish to run the network for.\n",
    "\n",
    "`log_interval`: This parameter sets the epoch interval after which a copy of the model weights will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "log_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in img_ids:\n",
    "    id_split = ids.split()\n",
    "    id_map[id_split[0]] = int(id_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sw/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2745: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/sw/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              4502528   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1025000   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 8,676,328\n",
      "Trainable params: 8,676,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(num_hidden_units, input_dim=word2vec_dim+img_dim, kernel_initializer='uniform'))\n",
    "model.add(Dropout(dropout))\n",
    "for i in range(num_hidden_layers):\n",
    "    model.add(Dense(num_hidden_units, kernel_initializer='uniform'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout))\n",
    "model.add(Dense(nb_classes, kernel_initializer='uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "#tensorboard = TensorBoard(log_dir='/output/Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3150"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dump = model.to_json()\n",
    "open('baseline_mlp'  + '.json', 'w').write(model_dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I've already performed these experiments once, so it'd be a nice idea to leverage the model I already created so here I've loaded the weights saved after the 99th epoch during my training experiment, and simply retrain those!\n",
    "\n",
    "### You may **skip** this step if you wish to build your model from scratch!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.load_weights('/floyd/input/vqa_data/weights/MLP_epoch_99.hdf5')\n",
    "print (\"Model Weights Loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And we're good to go!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.31 s, sys: 116 ms, total: 2.43 s\n",
      "Wall time: 1.8 s\n",
      "CPU times: user 1.91 ms, sys: 307 µs, total: 2.22 ms\n",
      "Wall time: 1.86 ms\n",
      "CPU times: user 1.61 s, sys: 224 ms, total: 1.83 s\n",
      "Wall time: 1.17 s\n",
      "   128/215407 [..............................] - ETA: 5011s - train loss: 7.2509CPU times: user 2.29 s, sys: 119 ms, total: 2.41 s\n",
      "Wall time: 1.81 s\n",
      "CPU times: user 2.02 ms, sys: 485 µs, total: 2.5 ms\n",
      "Wall time: 1.75 ms\n",
      "CPU times: user 598 ms, sys: 136 ms, total: 734 ms\n",
      "Wall time: 216 ms\n",
      "   256/215407 [..............................] - ETA: 4219s - train loss: 6.5618CPU times: user 2.33 s, sys: 122 ms, total: 2.45 s\n",
      "Wall time: 1.8 s\n",
      "CPU times: user 2.05 ms, sys: 470 µs, total: 2.52 ms\n",
      "Wall time: 1.74 ms\n",
      "CPU times: user 602 ms, sys: 128 ms, total: 730 ms\n",
      "Wall time: 218 ms\n",
      "   384/215407 [..............................] - ETA: 3946s - train loss: 6.7655CPU times: user 2.33 s, sys: 122 ms, total: 2.45 s\n",
      "Wall time: 1.9 s\n",
      "CPU times: user 2 ms, sys: 462 µs, total: 2.46 ms\n",
      "Wall time: 1.76 ms\n",
      "CPU times: user 599 ms, sys: 127 ms, total: 726 ms\n",
      "Wall time: 262 ms\n",
      "   512/215407 [..............................] - ETA: 3869s - train loss: 6.8971CPU times: user 2.53 s, sys: 139 ms, total: 2.67 s\n",
      "Wall time: 2.16 s\n",
      "CPU times: user 3.32 ms, sys: 2.1 ms, total: 5.42 ms\n",
      "Wall time: 5.69 ms\n",
      "CPU times: user 617 ms, sys: 128 ms, total: 744 ms\n",
      "Wall time: 244 ms\n",
      "   640/215407 [..............................] - ETA: 3908s - train loss: 7.3372CPU times: user 2.36 s, sys: 121 ms, total: 2.48 s\n",
      "Wall time: 1.9 s\n",
      "CPU times: user 2.07 ms, sys: 478 µs, total: 2.55 ms\n",
      "Wall time: 1.77 ms\n",
      "CPU times: user 605 ms, sys: 141 ms, total: 746 ms\n",
      "Wall time: 217 ms\n",
      "   768/215407 [..............................] - ETA: 3848s - train loss: 7.8043CPU times: user 2.32 s, sys: 119 ms, total: 2.44 s\n",
      "Wall time: 1.77 s\n",
      "CPU times: user 2.09 ms, sys: 451 µs, total: 2.54 ms\n",
      "Wall time: 1.8 ms\n",
      "CPU times: user 607 ms, sys: 142 ms, total: 749 ms\n",
      "Wall time: 228 ms\n",
      "   896/215407 [..............................] - ETA: 3778s - train loss: 7.7243CPU times: user 2.37 s, sys: 124 ms, total: 2.49 s\n",
      "Wall time: 1.84 s\n",
      "CPU times: user 2.04 ms, sys: 488 µs, total: 2.53 ms\n",
      "Wall time: 1.76 ms\n",
      "CPU times: user 593 ms, sys: 131 ms, total: 723 ms\n",
      "Wall time: 257 ms\n",
      "  1024/215407 [..............................] - ETA: 3745s - train loss: 7.6563CPU times: user 2.46 s, sys: 135 ms, total: 2.59 s\n",
      "Wall time: 2.05 s\n",
      "CPU times: user 2.59 ms, sys: 2.04 ms, total: 4.63 ms\n",
      "Wall time: 4.42 ms\n",
      "CPU times: user 587 ms, sys: 125 ms, total: 712 ms\n",
      "Wall time: 271 ms\n",
      "  1152/215407 [..............................] - ETA: 3762s - train loss: 7.6341"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Work/Archives/Minor/vqa_floyd/src/utils.py\u001b[0m in \u001b[0;36mget_questions_sum\u001b[0;34m(questions, nlp)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mques_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mques_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    344\u001b[0m             raise ValueError(Errors.E088.format(length=len(text),\n\u001b[1;32m    345\u001b[0m                                                 max_length=self.max_length))\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.25 ms, sys: 226 µs, total: 2.47 ms\n",
      "Wall time: 1.92 ms\n",
      "CPU times: user 602 ms, sys: 122 ms, total: 724 ms\n",
      "Wall time: 281 ms\n",
      "  1280/215407 [..............................] - ETA: 3718s - train loss: 7.3880CPU times: user 2.52 s, sys: 137 ms, total: 2.66 s\n",
      "Wall time: 2.21 s\n",
      "CPU times: user 2.25 ms, sys: 585 µs, total: 2.84 ms\n",
      "Wall time: 1.93 ms\n",
      "CPU times: user 600 ms, sys: 136 ms, total: 735 ms\n",
      "Wall time: 236 ms\n",
      "  1408/215407 [..............................] - ETA: 3751s - train loss: 7.1890CPU times: user 2.3 s, sys: 121 ms, total: 2.43 s\n",
      "Wall time: 1.84 s\n",
      "CPU times: user 2.02 ms, sys: 455 µs, total: 2.47 ms\n",
      "Wall time: 1.72 ms\n",
      "CPU times: user 611 ms, sys: 138 ms, total: 750 ms\n",
      "Wall time: 220 ms\n",
      "  1536/215407 [..............................] - ETA: 3725s - train loss: 6.9902CPU times: user 2.69 s, sys: 149 ms, total: 2.84 s\n",
      "Wall time: 2.29 s\n",
      "CPU times: user 2.08 ms, sys: 557 µs, total: 2.64 ms\n",
      "Wall time: 2.33 ms\n",
      "CPU times: user 587 ms, sys: 137 ms, total: 724 ms\n",
      "Wall time: 313 ms\n",
      "  1664/215407 [..............................] - ETA: 3773s - train loss: 6.8785CPU times: user 2.52 s, sys: 141 ms, total: 2.66 s\n",
      "Wall time: 1.98 s\n",
      "CPU times: user 2.03 ms, sys: 477 µs, total: 2.5 ms\n",
      "Wall time: 1.73 ms\n",
      "CPU times: user 587 ms, sys: 130 ms, total: 717 ms\n",
      "Wall time: 236 ms\n",
      "  1792/215407 [..............................] - ETA: 3767s - train loss: 6.7643CPU times: user 2.36 s, sys: 127 ms, total: 2.49 s\n",
      "Wall time: 1.98 s\n",
      "CPU times: user 2.57 ms, sys: 593 µs, total: 3.16 ms\n",
      "Wall time: 2.27 ms\n",
      "CPU times: user 598 ms, sys: 130 ms, total: 727 ms\n",
      "Wall time: 253 ms\n",
      "  1920/215407 [..............................] - ETA: 3763s - train loss: 6.6711"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Work/Archives/Minor/vqa_floyd/src/utils.py\u001b[0m in \u001b[0;36mget_questions_sum\u001b[0;34m(questions, nlp)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mques_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mques_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__call__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.precompute_hiddens.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.venv/lib/python3.6/site-packages/spacy/_ml.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         Yf = self.ops.xp.dot(X,\n\u001b[0;32m--> 149\u001b[0;31m             self.W.reshape((self.nF*self.nO*self.nP, self.nI)).T)\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mYf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mYf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.67 ms, sys: 405 µs, total: 3.07 ms\n",
      "Wall time: 2.49 ms\n",
      "CPU times: user 604 ms, sys: 137 ms, total: 741 ms\n",
      "Wall time: 241 ms\n",
      "  2048/215407 [..............................] - ETA: 3655s - train loss: 6.5601CPU times: user 2.45 s, sys: 145 ms, total: 2.59 s\n",
      "Wall time: 2.09 s\n",
      "CPU times: user 1.9 ms, sys: 497 µs, total: 2.4 ms\n",
      "Wall time: 2.26 ms\n",
      "CPU times: user 603 ms, sys: 133 ms, total: 736 ms\n",
      "Wall time: 263 ms\n",
      "  2176/215407 [..............................] - ETA: 3671s - train loss: 6.4393CPU times: user 2.43 s, sys: 141 ms, total: 2.57 s\n",
      "Wall time: 1.94 s\n",
      "CPU times: user 2.06 ms, sys: 472 µs, total: 2.53 ms\n",
      "Wall time: 1.76 ms\n",
      "CPU times: user 605 ms, sys: 127 ms, total: 732 ms\n",
      "Wall time: 250 ms\n",
      "  2304/215407 [..............................] - ETA: 3668s - train loss: 6.3537CPU times: user 2.47 s, sys: 141 ms, total: 2.61 s\n",
      "Wall time: 2.05 s\n",
      "CPU times: user 3.27 ms, sys: 718 µs, total: 3.99 ms\n",
      "Wall time: 3.72 ms\n",
      "CPU times: user 589 ms, sys: 129 ms, total: 718 ms\n",
      "Wall time: 254 ms\n",
      "  2432/215407 [..............................] - ETA: 3676s - train loss: 6.2892CPU times: user 2.52 s, sys: 146 ms, total: 2.66 s\n",
      "Wall time: 2.1 s\n",
      "CPU times: user 1.87 ms, sys: 517 µs, total: 2.38 ms\n",
      "Wall time: 1.8 ms\n",
      "CPU times: user 592 ms, sys: 128 ms, total: 721 ms\n",
      "Wall time: 255 ms\n",
      "  2560/215407 [..............................] - ETA: 3687s - train loss: 6.2643CPU times: user 2.5 s, sys: 145 ms, total: 2.64 s\n",
      "Wall time: 2.08 s\n",
      "CPU times: user 2.05 ms, sys: 376 µs, total: 2.42 ms\n",
      "Wall time: 2.05 ms\n",
      "CPU times: user 599 ms, sys: 130 ms, total: 729 ms\n",
      "Wall time: 293 ms\n",
      "  2688/215407 [..............................] - ETA: 3698s - train loss: 6.1954CPU times: user 2.44 s, sys: 140 ms, total: 2.58 s\n",
      "Wall time: 2 s\n",
      "CPU times: user 2.37 ms, sys: 694 µs, total: 3.06 ms\n",
      "Wall time: 2.13 ms\n",
      "CPU times: user 607 ms, sys: 140 ms, total: 747 ms\n",
      "Wall time: 235 ms\n",
      "  2816/215407 [..............................] - ETA: 3697s - train loss: 6.1333CPU times: user 2.51 s, sys: 149 ms, total: 2.66 s\n",
      "Wall time: 2.01 s\n",
      "CPU times: user 2.55 ms, sys: 634 µs, total: 3.18 ms\n",
      "Wall time: 2.3 ms\n",
      "CPU times: user 612 ms, sys: 135 ms, total: 747 ms\n",
      "Wall time: 228 ms\n",
      "  2944/215407 [..............................] - ETA: 3697s - train loss: 6.0632CPU times: user 2.41 s, sys: 138 ms, total: 2.55 s\n",
      "Wall time: 2.01 s\n",
      "CPU times: user 2.05 ms, sys: 467 µs, total: 2.52 ms\n",
      "Wall time: 1.74 ms\n",
      "CPU times: user 586 ms, sys: 126 ms, total: 712 ms\n",
      "Wall time: 244 ms\n",
      "  3072/215407 [..............................] - ETA: 3697s - train loss: 6.0126CPU times: user 2.49 s, sys: 149 ms, total: 2.63 s\n",
      "Wall time: 2.01 s\n",
      "CPU times: user 2.91 ms, sys: 859 µs, total: 3.77 ms\n",
      "Wall time: 2.74 ms\n",
      "CPU times: user 587 ms, sys: 131 ms, total: 718 ms\n",
      "Wall time: 259 ms\n",
      "  3200/215407 [..............................] - ETA: 3699s - train loss: 5.9740"
     ]
    }
   ],
   "source": [
    "for k in range(num_epochs):\n",
    "    index_shuffle = list(range(len(training_questions)))\n",
    "    shuffle(index_shuffle)\n",
    "    training_questions = [training_questions[i] for i in index_shuffle]\n",
    "    answers_train = [answers_train[i] for i in index_shuffle]\n",
    "    images_train = [images_train[i] for i in index_shuffle]\n",
    "    progbar = generic_utils.Progbar(len(training_questions))\n",
    "    for ques_batch, ans_batch, im_batch in zip(grouped(training_questions, batch_size, \n",
    "                                                       fillvalue=training_questions[-1]), \n",
    "                                               grouped(answers_train, batch_size, \n",
    "                                                       fillvalue=answers_train[-1]), \n",
    "                                               grouped(images_train, batch_size, fillvalue=images_train[-1])):\n",
    "        %time X_ques_batch = get_questions_sum(ques_batch, nlp)\n",
    "        %time X_img_batch = get_images_matrix(im_batch, id_map, img_features)\n",
    "        X_batch = np.hstack((X_ques_batch, X_img_batch))\n",
    "        Y_batch = get_answers_sum(ans_batch, lbl)\n",
    "        #loss = model.train_on_batch(X_batch, Y_batch,callbacks= [tensorboard])\n",
    "        %time loss = model.train_on_batch(X_batch, Y_batch)\n",
    "        progbar.add(batch_size, values=[('train loss', loss)])\n",
    "\n",
    "    if k%log_interval == 0:\n",
    "        model.save_weights(\"weights/MLP\" + \"_epoch_{:02d}.hdf5\".format(k))\n",
    "model.save_weights(\"weights/MLP\" + \"_epoch_{:02d}.hdf5\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's evaluate our model!\n",
    "\n",
    "We're going to evalute our model on the validation set provided by the **VQA Dataset** which I've already preprocessed much like our training datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded with Weights\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              4502528   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 22706)             23273650  \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 22706)             0         \n",
      "=================================================================\n",
      "Total params: 30,924,978\n",
      "Trainable params: 30,924,978\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_from_json(open('baseline_mlp.json').read())\n",
    "model.load_weights('weights/MLP_epoch_99.hdf5') #Pass in your weights file\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print (\"Model Loaded with Weights\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the validation preprocessed data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs = open('preprocessed/v1/val_images_coco_id.txt','rb').read().decode('utf-8').splitlines()\n",
    "val_ques = open('preprocessed/v1/ques_val.txt','rb').read().decode('utf-8').splitlines()\n",
    "val_ans  = open('preprocessed/v1/answer_val.txt','rb').read().decode('utf-8').splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = pk.load(open('preprocessed/v1/label_encoder.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "batch_size = 128 \n",
    "\n",
    "#print (\"Word2Vec Loaded!\")\n",
    "\n",
    "widgets = ['Evaluating ', Percentage(), ' ', Bar(marker='#',left='[',right=']'), ' ', ETA()]\n",
    "pbar = ProgressBar(widgets=widgets)\n",
    "#i=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qu_batch,an_batch,im_batch in pbar(zip(grouped(val_ques, batch_size, fillvalue=val_ques[0]), grouped(val_ans, batch_size, fillvalue=val_ans[0]), grouped(val_imgs, batch_size, fillvalue=val_imgs[0]))):\n",
    "    X_q_batch = get_questions_matrix(qu_batch, nlp)\n",
    "    X_i_batch = get_images_matrix(im_batch, id_map, vgg_features)\n",
    "    X_batch = np.hstack((X_q_batch, X_i_batch))\n",
    "    y_predict = model.predict_classes(X_batch, verbose=0)\n",
    "    y_pred.extend(label_encoder.inverse_transform(y_predict))\n",
    "    #print (i,\"/\",len(val_ques))\n",
    "    #i+=1\n",
    "    #print(label_encoder.inverse_transform(y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  48.74\n"
     ]
    }
   ],
   "source": [
    "correct_val = 0.0\n",
    "total = 0\n",
    "f1 = open('res.txt','w')\n",
    "\n",
    "for pred, truth, ques, img in zip(y_pred, val_ans, val_ques, val_imgs):\n",
    "    t_count = 0\n",
    "    for _truth in truth.split(';'):\n",
    "        if pred == truth:\n",
    "            t_count += 1 \n",
    "    if t_count >=2:\n",
    "        correct_val +=1\n",
    "    else:\n",
    "        correct_val += float(t_count)/3\n",
    "\n",
    "    total +=1\n",
    "\n",
    "    try:\n",
    "        f1.write(str(ques))\n",
    "        f1.write('\\n')\n",
    "        f1.write(str(img))\n",
    "        f1.write('\\n')\n",
    "        f1.write(str(pred))\n",
    "        f1.write('\\n')\n",
    "        f1.write(str(truth))\n",
    "        f1.write('\\n')\n",
    "        f1.write('\\n')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print (\"Accuracy: \", round((correct_val/total)*100,2)*)\n",
    "f1.write('Final Accuracy is ' + str(round(correct_val/total),2)*100)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go, all set to participate in the next VQA Challenge!\n",
    "\n",
    "If you do, however, would like to try out these models on your own custom images do checkout **`src/test.py`** with an image and a characterstic question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
