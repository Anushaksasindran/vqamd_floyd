{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Question Answering: Part I\n",
    "\n",
    "### Baseline Approach: A Bag of Words Model\n",
    "\n",
    "This notebook is simply an execution of the code to build VQA model using a basic `Neural Network (Multilayer Perceptron) + Bag of Words`, I would highly encourage you to read the [full post here](https://sominwadhwa.github.io/blog/2018/01/01/de/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/sominwadhwa/sominwadhwa.github.io/blob/master/assets/vqa/5.jpg?raw=true\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's get all the necessary library imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from random import shuffle, sample\n",
    "import pickle as pk\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from progressbar import Bar, ETA, Percentage, ProgressBar    \n",
    "from keras.models import model_from_json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import spacy\n",
    "#from spacy.en import English\n",
    "\n",
    "from src.utils import *\n",
    "from src.features import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessed Data\n",
    "\n",
    "The open-source VQA dataset contains multiple open-ended questions about various images. All my experiments were performed with v1 of the dataset (though I've processed v2 of the dataset as well), which contains:\n",
    "\n",
    "- 82,783 training images from COCO (common objects in context) dataset.\n",
    "- 215,407 question-answer pairs for training images.\n",
    "- 40,504 validation images to perform own testing.\n",
    "- 121,512 question-answer pairs for validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_questions = open(\"preprocessed/v1/ques_train.txt\",\"rb\").read().decode('utf8').splitlines()\n",
    "answers_train      = open(\"preprocessed/v1/answer_train.txt\",\"rb\").read().decode('utf8').splitlines()\n",
    "images_train       = open(\"preprocessed/v1/images_coco_id.txt\",\"rb\").read().decode('utf8').splitlines()\n",
    "img_ids            = open('preprocessed/v1/coco_vgg_IDMap.txt').read().splitlines()\n",
    "vgg_path           = \"data/coco/vgg_feats.mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple of questions along with their answers. The first entry you see here is the **COCO Image ID** through with the image can be found at [http://cocodataset.org/#explore](http://cocodataset.org/#explore) by simply entering the image ID in the **search** column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('284348', 'What is the color of the sweater?', 'red'),\n",
       " ('483557', 'Is this in America?', 'no'),\n",
       " ('501369', 'What is the building behind?', 'stop sign'),\n",
       " ('394071',\n",
       "  'Will the woman answer negatively if asked if its warmer than 35 degrees?',\n",
       "  'no'),\n",
       " ('568880', 'What condiment is on the table?', 'ketchup')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(list(zip(images_train, training_questions, answers_train)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 853 ms, sys: 103 ms, total: 956 ms\n",
      "Wall time: 958 ms\n",
      "Loaded WordVec\n"
     ]
    }
   ],
   "source": [
    "%time nlp = spacy.load(\"en\")\n",
    "print (\"Loaded WordVec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load image features - `4096` sized vectors extracted from the last layer of a VGG network trained on the COCO Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.68 s, sys: 2.49 s, total: 12.2 s\n",
      "Wall time: 12.3 s\n",
      "Loaded VGG Weights\n"
     ]
    }
   ],
   "source": [
    "%time vgg_features = scipy.io.loadmat(vgg_path)\n",
    "img_features = vgg_features['feats']\n",
    "id_map = dict()\n",
    "print (\"Loaded VGG Weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215407 215407 215407\n"
     ]
    }
   ],
   "source": [
    "upper_lim = 1000 #Number of most frequently occurring answers in COCOVQA (Coverting >85% of the total data)\n",
    "training_questions, answers_train, images_train = freq_answers(training_questions, answers_train, images_train, upper_lim)\n",
    "print (len(training_questions), len(answers_train),len(images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = LabelEncoder()\n",
    "lbl.fit(answers_train)\n",
    "nb_classes = len(list(lbl.classes_))\n",
    "pk.dump(lbl, open('preprocessed/v1/label_encoder_mlp.sav','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_units  = 1024\n",
    "num_hidden_layers = 3\n",
    "batch_size        = 128\n",
    "dropout           = 0.5\n",
    "activation        = 'tanh'\n",
    "img_dim           = 4096\n",
    "word2vec_dim      = 384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_epochs`: Set to the number of epochs you'd wish to run the network for.\n",
    "\n",
    "`log_interval`: This parameter sets the epoch interval after which a copy of the model weights will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "log_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in img_ids:\n",
    "    id_split = ids.split()\n",
    "    id_map[id_split[0]] = int(id_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sw/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2745: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/sw/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              4588544   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1025000   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 8,762,344\n",
      "Trainable params: 8,762,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(num_hidden_units, input_dim=word2vec_dim+img_dim, kernel_initializer='uniform'))\n",
    "model.add(Dropout(dropout))\n",
    "for i in range(num_hidden_layers):\n",
    "    model.add(Dense(num_hidden_units, kernel_initializer='uniform'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout))\n",
    "model.add(Dense(nb_classes, kernel_initializer='uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "#tensorboard = TensorBoard(log_dir='/output/Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3150"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dump = model.to_json()\n",
    "open('baseline_mlp'  + '.json', 'w').write(model_dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I've already performed these experiments once, so it'd be a nice idea to leverage the model I already created so here I've loaded the weights saved after the 99th epoch during my training experiment, and simply retrain those!\n",
    "\n",
    "### You may **skip** this step if you wish to build your model from scratch!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.load_weights('/floyd/input/vqa_data/weights/MLP_epoch_99.hdf5')\n",
    "print (\"Model Weights Loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And we're good to go!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.18 s, sys: 102 ms, total: 2.28 s\n",
      "Wall time: 1.7 s\n",
      "CPU times: user 2.04 ms, sys: 359 µs, total: 2.4 ms\n",
      "Wall time: 2.03 ms\n",
      "CPU times: user 1.63 s, sys: 231 ms, total: 1.86 s\n",
      "Wall time: 1.19 s\n",
      "   128/215407 [..............................] - ETA: 4866s - train loss: 7.1662CPU times: user 2.17 s, sys: 110 ms, total: 2.28 s\n",
      "Wall time: 1.68 s\n",
      "CPU times: user 2.02 ms, sys: 424 µs, total: 2.45 ms\n",
      "Wall time: 1.8 ms\n",
      "CPU times: user 625 ms, sys: 131 ms, total: 757 ms\n",
      "Wall time: 220 ms\n",
      "   256/215407 [..............................] - ETA: 4034s - train loss: 6.4516CPU times: user 2.22 s, sys: 115 ms, total: 2.33 s\n",
      "Wall time: 1.69 s\n",
      "CPU times: user 2.34 ms, sys: 359 µs, total: 2.7 ms\n",
      "Wall time: 2.02 ms\n",
      "CPU times: user 586 ms, sys: 116 ms, total: 702 ms\n",
      "Wall time: 243 ms\n",
      "   384/215407 [..............................] - ETA: 3776s - train loss: 6.9844CPU times: user 2.19 s, sys: 111 ms, total: 2.3 s\n",
      "Wall time: 1.68 s\n",
      "CPU times: user 1.95 ms, sys: 347 µs, total: 2.3 ms\n",
      "Wall time: 1.91 ms\n",
      "CPU times: user 611 ms, sys: 123 ms, total: 735 ms\n",
      "Wall time: 218 ms\n",
      "   512/215407 [..............................] - ETA: 3633s - train loss: 8.0453CPU times: user 2.23 s, sys: 119 ms, total: 2.35 s\n",
      "Wall time: 1.72 s\n",
      "CPU times: user 1.98 ms, sys: 437 µs, total: 2.41 ms\n",
      "Wall time: 1.8 ms\n",
      "CPU times: user 626 ms, sys: 136 ms, total: 762 ms\n",
      "Wall time: 225 ms\n",
      "   640/215407 [..............................] - ETA: 3562s - train loss: 8.7505CPU times: user 2.34 s, sys: 128 ms, total: 2.47 s\n",
      "Wall time: 1.97 s\n",
      "CPU times: user 2.38 ms, sys: 557 µs, total: 2.94 ms\n",
      "Wall time: 2.12 ms\n",
      "CPU times: user 617 ms, sys: 140 ms, total: 757 ms\n",
      "Wall time: 230 ms\n",
      "   768/215407 [..............................] - ETA: 3586s - train loss: 8.5093CPU times: user 2.18 s, sys: 111 ms, total: 2.29 s\n",
      "Wall time: 1.71 s\n",
      "CPU times: user 2.3 ms, sys: 513 µs, total: 2.81 ms\n",
      "Wall time: 1.92 ms\n",
      "CPU times: user 617 ms, sys: 140 ms, total: 758 ms\n",
      "Wall time: 224 ms\n",
      "   896/215407 [..............................] - ETA: 3537s - train loss: 8.2223CPU times: user 2.22 s, sys: 115 ms, total: 2.34 s\n",
      "Wall time: 1.73 s\n",
      "CPU times: user 2.01 ms, sys: 381 µs, total: 2.39 ms\n",
      "Wall time: 1.75 ms\n",
      "CPU times: user 607 ms, sys: 128 ms, total: 735 ms\n",
      "Wall time: 232 ms\n",
      "  1024/215407 [..............................] - ETA: 3506s - train loss: 8.2718CPU times: user 2.43 s, sys: 132 ms, total: 2.56 s\n",
      "Wall time: 2.1 s\n",
      "CPU times: user 1.96 ms, sys: 438 µs, total: 2.4 ms\n",
      "Wall time: 1.78 ms\n",
      "CPU times: user 608 ms, sys: 119 ms, total: 727 ms\n",
      "Wall time: 242 ms\n",
      "  1152/215407 [..............................] - ETA: 3552s - train loss: 8.2720CPU times: user 2.2 s, sys: 113 ms, total: 2.32 s\n",
      "Wall time: 1.71 s\n",
      "CPU times: user 2.46 ms, sys: 507 µs, total: 2.97 ms\n",
      "Wall time: 2.13 ms\n",
      "CPU times: user 616 ms, sys: 136 ms, total: 752 ms\n",
      "Wall time: 216 ms\n",
      "  1280/215407 [..............................] - ETA: 3518s - train loss: 7.9680CPU times: user 2.2 s, sys: 119 ms, total: 2.32 s\n",
      "Wall time: 1.69 s\n",
      "CPU times: user 1.98 ms, sys: 441 µs, total: 2.42 ms\n",
      "Wall time: 1.79 ms\n",
      "CPU times: user 607 ms, sys: 131 ms, total: 738 ms\n",
      "Wall time: 215 ms\n",
      "  1408/215407 [..............................] - ETA: 3487s - train loss: 7.7380CPU times: user 2.14 s, sys: 118 ms, total: 2.26 s\n",
      "Wall time: 1.71 s\n",
      "CPU times: user 1.99 ms, sys: 427 µs, total: 2.42 ms\n",
      "Wall time: 1.79 ms\n",
      "CPU times: user 615 ms, sys: 132 ms, total: 748 ms\n",
      "Wall time: 232 ms\n",
      "  1536/215407 [..............................] - ETA: 3466s - train loss: 7.5446"
     ]
    }
   ],
   "source": [
    "for k in range(num_epochs):\n",
    "    index_shuffle = list(range(len(training_questions)))\n",
    "    shuffle(index_shuffle)\n",
    "    training_questions = [training_questions[i] for i in index_shuffle]\n",
    "    answers_train = [answers_train[i] for i in index_shuffle]\n",
    "    images_train = [images_train[i] for i in index_shuffle]\n",
    "    progbar = generic_utils.Progbar(len(training_questions))\n",
    "    for ques_batch, ans_batch, im_batch in zip(grouped(training_questions, batch_size, \n",
    "                                                       fillvalue=training_questions[-1]), \n",
    "                                               grouped(answers_train, batch_size, \n",
    "                                                       fillvalue=answers_train[-1]), \n",
    "                                               grouped(images_train, batch_size, fillvalue=images_train[-1])):\n",
    "        X_ques_batch = get_questions_sum(ques_batch, nlp)\n",
    "        X_img_batch = get_images_matrix(im_batch, id_map, img_features)\n",
    "        X_batch = np.hstack((X_ques_batch, X_img_batch))\n",
    "        Y_batch = get_answers_sum(ans_batch, lbl)\n",
    "        #loss = model.train_on_batch(X_batch, Y_batch,callbacks= [tensorboard])\n",
    "        loss = model.train_on_batch(X_batch, Y_batch)\n",
    "        progbar.add(batch_size, values=[('train loss', loss)])\n",
    "\n",
    "    if k%log_interval == 0:\n",
    "        model.save_weights(\"weights/MLP\" + \"_epoch_{:02d}.hdf5\".format(k))\n",
    "model.save_weights(\"weights/MLP\" + \"_epoch_{:02d}.hdf5\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's evaluate our model!\n",
    "\n",
    "We're going to evalute our model on the validation set provided by the **VQA Dataset** which I've already preprocessed much like our training datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded with Weights\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              4502528   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 22706)             23273650  \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 22706)             0         \n",
      "=================================================================\n",
      "Total params: 30,924,978\n",
      "Trainable params: 30,924,978\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_from_json(open('baseline_mlp.json').read())\n",
    "model.load_weights('weights/MLP_epoch_99.hdf5') #Pass in your weights file\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print (\"Model Loaded with Weights\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the validation preprocessed data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs = open('preprocessed/v1/val_images_coco_id.txt','rb').read().decode('utf-8').splitlines()\n",
    "val_ques = open('preprocessed/v1/ques_val.txt','rb').read().decode('utf-8').splitlines()\n",
    "val_ans  = open('preprocessed/v1/answer_val.txt','rb').read().decode('utf-8').splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = pk.load(open('preprocessed/v1/label_encoder.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "batch_size = 128 \n",
    "\n",
    "#print (\"Word2Vec Loaded!\")\n",
    "\n",
    "widgets = ['Evaluating ', Percentage(), ' ', Bar(marker='#',left='[',right=']'), ' ', ETA()]\n",
    "pbar = ProgressBar(widgets=widgets)\n",
    "#i=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qu_batch,an_batch,im_batch in pbar(zip(grouped(val_ques, batch_size, fillvalue=val_ques[0]), grouped(val_ans, batch_size, fillvalue=val_ans[0]), grouped(val_imgs, batch_size, fillvalue=val_imgs[0]))):\n",
    "    X_q_batch = get_questions_matrix(qu_batch, nlp)\n",
    "    X_i_batch = get_images_matrix(im_batch, id_map, vgg_features)\n",
    "    X_batch = np.hstack((X_q_batch, X_i_batch))\n",
    "    y_predict = model.predict_classes(X_batch, verbose=0)\n",
    "    y_pred.extend(label_encoder.inverse_transform(y_predict))\n",
    "    #print (i,\"/\",len(val_ques))\n",
    "    #i+=1\n",
    "    #print(label_encoder.inverse_transform(y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  48.74\n"
     ]
    }
   ],
   "source": [
    "correct_val = 0.0\n",
    "total = 0\n",
    "f1 = open('res.txt','w')\n",
    "\n",
    "for pred, truth, ques, img in zip(y_pred, val_ans, val_ques, val_imgs):\n",
    "    t_count = 0\n",
    "    for _truth in truth.split(';'):\n",
    "        if pred == truth:\n",
    "            t_count += 1 \n",
    "    if t_count >=2:\n",
    "        correct_val +=1\n",
    "    else:\n",
    "        correct_val += float(t_count)/3\n",
    "\n",
    "    total +=1\n",
    "\n",
    "    try:\n",
    "        f1.write(str(ques))\n",
    "        f1.write('\\n')\n",
    "        f1.write(str(img))\n",
    "        f1.write('\\n')\n",
    "        f1.write(str(pred))\n",
    "        f1.write('\\n')\n",
    "        f1.write(str(truth))\n",
    "        f1.write('\\n')\n",
    "        f1.write('\\n')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print (\"Accuracy: \", round((correct_val/total)*100,2)*)\n",
    "f1.write('Final Accuracy is ' + str(round(correct_val/total),2)*100)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go, all set to participate in the next VQA Challenge!\n",
    "\n",
    "If you do, however, would like to try out these models on your own custom images do checkout **`src/test.py`** with an image and a characterstic question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
