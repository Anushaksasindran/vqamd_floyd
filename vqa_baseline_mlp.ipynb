{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you haven't, please execute the following cell **once per Workspace Session** to install all the necessary requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash run_me_first_on_floyd.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Question Answering: Part I\n",
    "\n",
    "### Baseline Approach: A Bag of Words Model\n",
    "\n",
    "This notebook is simply an execution of the code to build VQA model using a basic `Neural Network (Multilayer Perceptron) + Bag of Words`, I would highly encourage you to read the [full post here](https://sominwadhwa.github.io/blog/2018/01/01/de/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/sominwadhwa/sominwadhwa.github.io/blob/master/assets/vqa/5.jpg?raw=true\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's get all the necessary library imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from random import shuffle, sample\n",
    "import pickle as pk\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from progressbar import Bar, ETA, Percentage, ProgressBar    \n",
    "from keras.models import model_from_json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import spacy\n",
    "#from spacy.en import English\n",
    "\n",
    "from src.utils import *\n",
    "from src.features import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessed Data\n",
    "\n",
    "The open-source VQA dataset contains multiple open-ended questions about various images. All my experiments were performed with v1 of the dataset (though I've processed v2 of the dataset as well), which contains:\n",
    "\n",
    "- 82,783 training images from COCO (common objects in context) dataset.\n",
    "- 215,407 question-answer pairs for training images.\n",
    "- 40,504 validation images to perform own testing.\n",
    "- 121,512 question-answer pairs for validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_questions = open(\"preprocessed/v1/ques_train.txt\",\"rb\").read().decode('utf8').splitlines()\n",
    "answers_train      = open(\"preprocessed/v1/answer_train.txt\",\"rb\").read().decode('utf8').splitlines()\n",
    "images_train       = open(\"preprocessed/v1/images_coco_id.txt\",\"rb\").read().decode('utf8').splitlines()\n",
    "img_ids            = open('preprocessed/v1/coco_vgg_IDMap.txt').read().splitlines()\n",
    "vgg_path           = \"/floyd/input/vqa_data/coco/vgg_feats.mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple of questions along with their answers. The first entry you see here is the **COCO Image ID** through with the image can be found at [http://cocodataset.org/#explore](http://cocodataset.org/#explore) by simply entering the image ID in the **search** column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('74523', 'What word is printed on the road?', 'stop'),\n",
       " ('378471', 'Is this man talking on his cell phone?', 'yes'),\n",
       " ('120655', 'What is covering the ground?', 'snow'),\n",
       " ('64557', 'Is the light on?', 'yes'),\n",
       " ('463434', 'Are there cars on the bridge?', 'no')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(list(zip(images_train, training_questions, answers_train)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded WordVec\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "print (\"Loaded WordVec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load image features - `4096` sized vectors extracted from the last layer of a VGG network trained on the COCO Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 s, sys: 3.07 s, total: 13.3 s\n",
      "Wall time: 13.8 s\n",
      "Loaded VGG Weights\n"
     ]
    }
   ],
   "source": [
    "%time vgg_features = scipy.io.loadmat(vgg_path)\n",
    "img_features = vgg_features['feats']\n",
    "id_map = dict()\n",
    "print (\"Loaded VGG Weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215407 215407 215407\n"
     ]
    }
   ],
   "source": [
    "upper_lim = 1000 #Number of most frequently occurring answers in COCOVQA (Coverting >85% of the total data)\n",
    "training_questions, answers_train, images_train = freq_answers(training_questions, answers_train, images_train, upper_lim)\n",
    "print (len(training_questions), len(answers_train),len(images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = LabelEncoder()\n",
    "lbl.fit(answers_train)\n",
    "nb_classes = len(list(lbl.classes_))\n",
    "pk.dump(lbl, open('preprocessed/v1/label_encoder_mlp.sav','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_units  = 1024\n",
    "num_hidden_layers = 3\n",
    "batch_size        = 256\n",
    "dropout           = 0.5\n",
    "activation        = 'tanh'\n",
    "img_dim           = 4096\n",
    "word2vec_dim      = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_epochs`: Set to the number of epochs you'd wish to run the network for.\n",
    "\n",
    "`log_interval`: This parameter sets the epoch interval after which a copy of the model weights will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "log_interval = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in img_ids:\n",
    "    id_split = ids.split()\n",
    "    id_map[id_split[0]] = int(id_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              4502528   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1025000   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 8,676,328\n",
      "Trainable params: 8,676,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(num_hidden_units, input_dim=word2vec_dim+img_dim, kernel_initializer='uniform'))\n",
    "model.add(Dropout(dropout))\n",
    "for i in range(num_hidden_layers):\n",
    "    model.add(Dense(num_hidden_units, kernel_initializer='uniform'))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout))\n",
    "model.add(Dense(nb_classes, kernel_initializer='uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "#tensorboard = TensorBoard(log_dir='/output/Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3290"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dump = model.to_json()\n",
    "open('baseline_mlp'  + '.json', 'w').write(model_dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I've already performed these experiments once, so it'd be a nice idea to leverage the model I already created so here I've loaded the weights saved after the 99th epoch during my training experiment, and simply retrain those!\n",
    "\n",
    "### You may **skip** this step if you wish to build your model from scratch!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.load_weights('/floyd/input/vqa_data/weights/MLP_epoch_25.hdf5')\n",
    "print (\"Model Weights Loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And we're good to go!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215552/215407 [==============================] - 98s 456us/step - train loss: 4.3291\n",
      "215552/215407 [==============================] - 95s 438us/step - train loss: 3.3275\n",
      "215552/215407 [==============================] - 94s 437us/step - train loss: 3.1561\n",
      "215552/215407 [==============================] - 94s 436us/step - train loss: 3.0834\n",
      "215552/215407 [==============================] - 95s 441us/step - train loss: 3.0399\n",
      "215552/215407 [==============================] - 96s 443us/step - train loss: 3.0026\n",
      "215552/215407 [==============================] - 92s 427us/step - train loss: 2.9732\n",
      "215552/215407 [==============================] - 92s 425us/step - train loss: 2.9587\n",
      "215552/215407 [==============================] - 91s 423us/step - train loss: 2.9400\n",
      "215552/215407 [==============================] - 92s 429us/step - train loss: 2.9426\n",
      "215552/215407 [==============================] - 94s 437us/step - train loss: 2.9338\n",
      "215552/215407 [==============================] - 93s 433us/step - train loss: 2.9314\n",
      "215552/215407 [==============================] - 92s 427us/step - train loss: 2.9381\n",
      "215552/215407 [==============================] - 91s 422us/step - train loss: 2.9273\n",
      "215552/215407 [==============================] - 92s 426us/step - train loss: 2.9185\n",
      "215552/215407 [==============================] - 93s 431us/step - train loss: 2.9165\n",
      "215552/215407 [==============================] - 93s 430us/step - train loss: 2.9271\n",
      "215552/215407 [==============================] - 92s 425us/step - train loss: 2.9332\n",
      "215552/215407 [==============================] - 92s 426us/step - train loss: 2.9300\n",
      "215552/215407 [==============================] - 91s 424us/step - train loss: 2.9334\n",
      "215552/215407 [==============================] - 92s 425us/step - train loss: 2.9386\n",
      "215552/215407 [==============================] - 92s 425us/step - train loss: 2.9556\n",
      "215552/215407 [==============================] - 93s 429us/step - train loss: 2.9515\n",
      "215552/215407 [==============================] - 92s 425us/step - train loss: 2.9427\n",
      "215552/215407 [==============================] - 91s 421us/step - train loss: 2.9468\n",
      "215552/215407 [==============================] - 91s 421us/step - train loss: 2.9561\n",
      "215552/215407 [==============================] - 92s 428us/step - train loss: 2.9521\n",
      "215552/215407 [==============================] - 93s 432us/step - train loss: 2.9582\n",
      "215552/215407 [==============================] - 93s 434us/step - train loss: 2.9681\n",
      "215552/215407 [==============================] - 93s 431us/step - train loss: 2.9657\n",
      " 33280/215407 [===>..........................] - ETA: 1:18 - train loss: 2.9500"
     ]
    }
   ],
   "source": [
    "for k in range(num_epochs):\n",
    "    index_shuffle = list(range(len(training_questions)))\n",
    "    shuffle(index_shuffle)\n",
    "    training_questions = [training_questions[i] for i in index_shuffle]\n",
    "    answers_train = [answers_train[i] for i in index_shuffle]\n",
    "    images_train = [images_train[i] for i in index_shuffle]\n",
    "    progbar = generic_utils.Progbar(len(training_questions))\n",
    "    for ques_batch, ans_batch, im_batch in zip(grouped(training_questions, batch_size, \n",
    "                                                       fillvalue=training_questions[-1]), \n",
    "                                               grouped(answers_train, batch_size, \n",
    "                                                       fillvalue=answers_train[-1]), \n",
    "                                               grouped(images_train, batch_size, fillvalue=images_train[-1])):\n",
    "        X_ques_batch = get_questions_sum(ques_batch, nlp)\n",
    "        X_img_batch = get_images_matrix(im_batch, id_map, img_features)\n",
    "        X_batch = np.hstack((X_ques_batch, X_img_batch))\n",
    "        Y_batch = get_answers_sum(ans_batch, lbl)\n",
    "        #loss = model.train_on_batch(X_batch, Y_batch,callbacks= [tensorboard])\n",
    "        loss = model.train_on_batch(X_batch, Y_batch)\n",
    "        progbar.add(batch_size, values=[('train loss', loss)])\n",
    "\n",
    "    if k%log_interval == 0:\n",
    "        model.save_weights(\"weights/MLP\" + \"_epoch_{:02d}.hdf5\".format(k))\n",
    "model.save_weights(\"weights/MLP\" + \"_epoch_{:02d}.hdf5\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's evaluate our model!\n",
    "\n",
    "We're going to evalute our model on the validation set provided by the **VQA Dataset** which I've already preprocessed much like our training datasets. Refer to [VQA Evaluation](http://visualqa.org/evaluation.html).\n",
    "\n",
    "While I have evaluated my pre-trained models over here, you might like to change the paths in order to evaluate your own models. This can be easily done in the following way -\n",
    "\n",
    "1. Add `model_from_json(open('lstm_structure.json').read())` (instead of loading model structure from my dataset, use the one you just created.\n",
    "2. Modify -> `model.load_weights(\"weights/<weights_file>\")` (instead of loading weights from my pretrained models, use your own set stored under `weights` directory.\n",
    "\n",
    "By default, we're going to load the weights & the model created at the beginning of your training loop (for testing purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sw/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2745: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/sw/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Model Loaded with Weights\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              4502528   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1025000   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 8,676,328\n",
      "Trainable params: 8,676,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_from_json(open('baseline_mlp.json').read())\n",
    "# In case you wish to evaluate the model you just trained, uncomment the following line of code & comment out the subsequent one -\n",
    "#model.load_weights('weights/MLP_epoch_25.hdf5')\n",
    "model.load_weights('/floyd/input/vqa_data/weights/MLP_epoch_25.hdf5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print (\"Model Loaded with Weights\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the validation preprocessed data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs = open('preprocessed/v1/val_images_coco_id.txt','rb').read().decode('utf-8').splitlines()\n",
    "val_ques = open('preprocessed/v1/ques_val.txt','rb').read().decode('utf-8').splitlines()\n",
    "val_ans  = open('preprocessed/v1/answer_val.txt','rb').read().decode('utf-8').splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace location of LabelEncoder to your own, otherwise this may affect accuracy.** To do so, simply change the `file_path` to `preprocessed/v1/label_encoder_<type>.sav`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = pk.load(open('preprocessed/v1/label_encoder_mlp.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "batch_size = 128 \n",
    "\n",
    "#print (\"Word2Vec Loaded!\")\n",
    "\n",
    "widgets = ['Evaluating ', Percentage(), ' ', Bar(marker='#',left='[',right=']'), ' ', ETA()]\n",
    "pbar = ProgressBar(widgets=widgets)\n",
    "#i=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating N/A% [#                                             ] Time:  0:03:26\n"
     ]
    }
   ],
   "source": [
    "for qu_batch,an_batch,im_batch in pbar(zip(grouped(val_ques, batch_size, fillvalue=val_ques[0]), grouped(val_ans, batch_size, fillvalue=val_ans[0]), grouped(val_imgs, batch_size, fillvalue=val_imgs[0]))):\n",
    "    X_q_batch = get_questions_matrix(qu_batch, nlp)\n",
    "    X_i_batch = get_images_matrix(im_batch, id_map, img_features)\n",
    "    X_batch = np.hstack((X_q_batch, X_i_batch))\n",
    "    y_predict = model.predict_classes(X_batch, verbose=0)\n",
    "    y_pred.extend(label_encoder.inverse_transform(y_predict))\n",
    "    #print (i,\"/\",len(val_ques))\n",
    "    #i+=1\n",
    "    #print(label_encoder.inverse_transform(y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  48.21\n"
     ]
    }
   ],
   "source": [
    "correct_val = 0.0\n",
    "total = 0\n",
    "f1 = open('res.txt','w')\n",
    "\n",
    "for pred, truth, ques, img in zip(y_pred, val_ans, val_ques, val_imgs):\n",
    "    t_count = 0\n",
    "    for _truth in truth.split(';'):\n",
    "        if pred == truth:\n",
    "            t_count += 1 \n",
    "    if t_count >=2:\n",
    "        correct_val +=1\n",
    "    else:\n",
    "        correct_val += float(t_count)/3\n",
    "\n",
    "    total +=1\n",
    "\n",
    "    try:\n",
    "        f1.write(str(ques))\n",
    "        f1.write('\\n')\n",
    "        f1.write(str(img))\n",
    "        f1.write('\\n')\n",
    "        f1.write(str(pred))\n",
    "        f1.write('\\n')\n",
    "        f1.write(str(truth))\n",
    "        f1.write('\\n')\n",
    "        f1.write('\\n')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print (\"Accuracy: \", round((correct_val/total)*100,2))\n",
    "#f1.write('Final Accuracy is ' + str(round(correct_val/total),2)*100)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to try out the VQA Model?\n",
    "\n",
    "**Execute to `src/test.py` with any of the sample images provided with a question of your choice!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go, all set to participate in the next VQA Challenge!\n",
    "\n",
    "If you do, however, would like to try out these models on your own custom images do checkout **`src/test.py`** with an image and a characterstic question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
